{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fa5b2d6",
   "metadata": {},
   "source": [
    "$123456789 \\times 987654321 =$ ?\n",
    "\n",
    "How fast you can compute this using the logic learned in the school?\n",
    "\n",
    "* Input: two n-digit numbers x and y\n",
    "* Output: the product x.y\n",
    "\n",
    "Consider two numbers  $ùë•=5678$ and  $ùë¶=1234$ \\\n",
    "Here, the number of digits in  $ùë•$ and  $ùë¶$ is  $ùëõ=4$ \\\n",
    "The algorithm works by taking partial products between the numbers, say  $5678\\times 4$, $5678\\times 3$,  $5678 \\times 2$  and  $5678\\times 1$\\\n",
    "By doing this, we add the carries as necessary\n",
    "\n",
    "#### How do we measure the running time of this algorithm?\n",
    "* Is it a good option to measure the running time of an algorithm using seconds/milli seconds? \n",
    "* Does it give you a clear understanding about the running time of an algorithm?\n",
    "* Measuring the running time of an algorithm using seconds/milli seconds gives you the running of that algorithm with a particular implementation, on a particular piece of hardware.\n",
    "* For example, school multiplication algorithm is much faster on a computer than by hand, but it‚Äôs still the same algorithm in both cases.\n",
    "\n",
    "<img src=images/fig1-integer-multiplication.png width=\"800\" height=\"800\">\n",
    "\n",
    "#### Analysis of the Number of Operations\n",
    "*  The performance of this algorithm can be analysed by counting the number of primitive operations needed to carry out this operation.\n",
    "* In the first partial product  $5678\\times 4$, each of the digits  5,6,7,8  are multiplied 4 times with the first number (4). This is  4 primitive operations.\n",
    "* Performed a  3 additions because of the carries.\n",
    "* Computing a partial product involves  $ùëõ$ multiplications (one per digit) and at most  $ùëõ$ additions (at most one per digit), for a total of at most  $2ùëõ$ primitive operations. Every partial product requires at most  $2ùëõ$ operations.\n",
    "* It requires at most  $ùëõ\\times 2ùëõ=2ùëõ^2$ primitive operations.\n",
    "* Adding all partial products up to compute the final answer takes a comparable number of operations (at most another $2ùëõ^2$).\n",
    "\n",
    "<span style=\"color:red\">Is it possible to design an algorithm with lesser number of operations?</span>\n",
    "\n",
    "* Consider two algorithms: one with a running time that increases proportionally to the square of the input size ( $ùëõ^2$), and another with a running time that increases proportionally to the $1.6^{th}$ power of the input size ($ùëõ^{1.6}$)\n",
    "* It would be more efficient to manually execute the  $ùëõ^{1.6}$ algorithm rather than running the  $ùëõ^2$ algorithm on a computer. \n",
    "* $n^{1.6}$ approach is \"faster\" since, regardless of the implementation or hardware, it will consistently outperform other algorithms for big values of  $ùëõ$\n",
    "\n",
    "### MergeSort\n",
    "* Problem: Sorting\n",
    "* Input: An array of $n$ numbers, in arbitrary order\n",
    "* Output: An array of the same numbers, sorted from smallest to largest\n",
    "\n",
    "<img src=images/merge-sort.png width=\"800\" height=\"800\">\n",
    "<img src=images/09fig18.jpg width=\"500\" height=\"500\">\n",
    "\n",
    "###### Divide and Conquer\n",
    "* As a recursive divide-and conquer algorithm, MergeSort calls itself on smaller arrays. \n",
    "* The simplest way to decompose a sorting problem into smaller sorting problems is to break the input array in half.\n",
    "* The first and second halves are each sorted recursively.\n",
    "<img src=images/msort-1.png width=\"600\" height=\"600\">\n",
    "<img src=images/msort-2.png width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "* What‚Äôs the running time of the MergeSort algorithm, as a function of the length n of the input array?\n",
    "* Is it faster than more straightforward methods of sorting, such as Selection sort, Insertion sort, and Bubble sort?\n",
    "\n",
    "How many operations are performed in a single invocation of the Merge subroutine when called on two arrays of length ($\\frac{n}{2}$) each?\n",
    "\n",
    "* Lines 1 and 2 each perform an initialization, and we‚Äôll count this as two operations.\n",
    "* We have a for loop that executes a total of $n$ times\n",
    "* Each iteration of the loop performs\n",
    "    * A comparison in line 4\n",
    "    * An assignment in either line 5 or line 8\n",
    "    * An increment in either line 6 or line 9\n",
    "    * Loop index $k$ also gets incremented in each loop iteration\n",
    "    * 4 primitive operations are performed for each of the n iterations of the loop\n",
    "    \n",
    "*  Merge subroutine performs at most $4n+ 2$ operations to merge two sorted arrays of length $\\frac{n}{2}$ each\n",
    "\n",
    "* **Total number of operations When $n\\ge 1$, $4n+2\\le6n$**\n",
    "\n",
    "**<span style=\"color:red\">For every input array of length $n \\ge 1$, the MergeSort algorithm performs at most $6nlog_2n+6n$ operations</span>**\n",
    "##### Proof\n",
    "* For simplicity, we assume that the input array length $n$ is a power of 2.\n",
    "* Proof requires building a recursion tree \n",
    "* The idea of the recursion tree method is to\n",
    "    * write out all the work done by a recursive algorithm in a tree structure, with nodes of the tree corresponding to recursive calls\n",
    "    * children of a node corresponding to the recursive calls made by that node\n",
    "    \n",
    "<img src=images/msort-3.png width=\"600\" height=\"600\">\n",
    "\n",
    "Nodes correspond to recursive calls. Level 0 corresponds to the outermost call to MergeSort, level 1 to its recursive calls, and so on.\n",
    "\n",
    "<img src=images/msort-4.png width=\"800\" height=\"800\">\n",
    "\n",
    "* Since each invocation of MergeSort spawns two recursive calls, the tree will be binary\n",
    "    * two children per node\n",
    "* Level 0  has two recursive calls\n",
    "* Level 1 of the tree has two nodes, corresponding to the two recursive calls made by the outermost call, one for the left half of the input array and one for the right half\n",
    "* Each of the level-1 recursive calls will itself make two recursive calls\n",
    "* This process continues until eventually the recursion bottoms out with arrays of size 1\n",
    "\n",
    "Need to understand two things:\n",
    "* The number of distinct subproblems at a given recursion level $j$\n",
    "* The length of the input to each of these subproblems.\n",
    "\n",
    "How much work is done by the level-$j$ recursive calls, not counting the work done by their recursive calls at later levels?\n",
    "* It does only three things\n",
    "    * Make two recursive calls\n",
    "    * Invoke the Merge subroutine on the results\n",
    "\n",
    "* We know that there are at most $6n$ operations, where $n$ is the length of the input array to this subproblem\n",
    "* We can observe that at the $j^{th}$ level, there will be $2^j$ subproblems, each with inputs of size $n/2^j$.\n",
    "\n",
    "Work at level $j$ = (number of subproblems)$\\times$(work per subproblem) $\\le 2^j\\times 6(n/2^j)$ = $6n$ operations\n",
    "\n",
    "* The recursion tree has $log_2n + 1$ levels (levels 0 through $log_2n$, inclusive)\n",
    "* Using our bound of $6n$ operations per level, we can bound the total number of operations by,\n",
    "\n",
    "Total no. of operations = number of levels $\\times$ work per level  $\\le (log_2n + 1).6n$ = $6n log_2n + 6n$\n",
    "\n",
    "## Guiding Principles for the Analysis of Algorithms\n",
    "\n",
    "Objective: **Determine an optimal point for the analysis of algorithms, which strikes a balance between precision and manageability.**\n",
    "\n",
    "* Precise examination of running time is only feasible for the most basic algorithms; in most cases, compromises are necessary.\n",
    "* Mathematical analysis should be employed to accurately predict the efficiency (fast or slow) of an algorithm in practical scenarios.\n",
    "\n",
    "### Principle #1: Worst-Case Analysis\n",
    "* The running time bound of $6n log_2 n + 6n$ for merge sort holds for every input array of length $n$, no matter what its contents.\n",
    "* No assumptions about the input beyond its length $n$ is made\n",
    "\n",
    "* Hypothetically, if there was an adversary whose sole purpose in life was to concoct a malevolent input designed to make MergeSort run as slow as possible, the $6n log_2 n + 6n$ bound would still apply.\n",
    "    * worst-case analysis\n",
    "    * It gives a running time bound that is valid even for the ‚Äúworst‚Äù inputs.\n",
    "\n",
    "### Principle #2: Big-Picture Analysis\n",
    "* Warning!!! - Big-picture analysis is not a technical term\n",
    "* It is unnecessary to excessively concern with insignificant constant factors or lower-order terms while considering the constraints of running time.\n",
    "* Constants depend on environment-specific factors\n",
    "* Lose little predictive power\n",
    "\n",
    "### Principle #3: Asymptotic Analysis\n",
    "* Focus on the rate at which an algorithm's running time increases as the size of the input, $n$, becomes larger\n",
    "* The preference for larger inputs was already apparent when we analysed the time complexity of MergeSort, which is $6n log_2n + 6n$ operations.\n",
    "\n",
    "<img src=images/comp-comparison.png width=\"600\" height=\"600\">\n",
    "$6n log_2n + 6n$ vs. $(1/2)n^2$: $(1/2)n^2$ is the smaller expression when $n$ is small, while $6n log_2n + 6n$ is smaller for all larger $n$.\n",
    "\n",
    "#### Why do we require asymptotic analysis?\n",
    "\n",
    "* Consider a predetermined time constraint, such as an hour or a day\n",
    "\n",
    "    * What is the relationship between the size of a solvable problem and the amount of processing power available?\n",
    "\n",
    "    * By employing an algorithm that runs in time proportional to the input size, a four-fold increase in computing power lets you solve problems four times as large as before. \n",
    "\n",
    "    * By employing an algorithm with a time complexity that is directly proportional to the square of the input size, you would have the capability to solve problems that are merely twice as large as those previously solvable.\n",
    "\n",
    "#### What Is a ‚ÄúFast‚Äù Algorithm?\n",
    "**A ‚Äúfast algorithm‚Äù is an algorithm whose worst-case running time grows slowly with the input size.**\n",
    "\n",
    "### Asymptotic Notation\n",
    "* Offers fundamental terminology for addressing the design and analysis of algorithms.\n",
    "* Understanding the meaning of \"big-O of n time\" and \"big-O of n-squared time\" is crucial for programmers.\n",
    "* Helps to disregard all the specific features:\n",
    "    * Selection of architecture\n",
    "    * Programming language\n",
    "    * Compiler\n",
    "    * and other factors.\n",
    "* Helps us to distinguish between more efficient and less efficient methods of solving one problem.\n",
    "\n",
    "<img src=images/comp-1(1).png width=\"600\" height=\"600\">\n",
    "\n",
    "\n",
    "**Complexity of MergeSort is $O(nlogn)$**\n",
    "* Omitted the terms from the expression $6nlogn+6n$ \n",
    "* The lower-order term $6n$ grows more slowly than $6nlogn$ \n",
    "* Constant factor of 6 also gets suppressed\n",
    "\n",
    "* So, running time of MergeSort is ‚Äúbig-O of $n log n$,‚Äù written $O(n log n)$ Or MergeSort is an $O(n log n)$-time algorithm\n",
    "\n",
    "To talk about the running time of algorithms, we will use the following notation. \n",
    "* $T(n)$ denotes the runtime of an algorithm on an input of size $n$.\n",
    "\n",
    "\n",
    "Asymptotic notation\n",
    "* Big-Oh Notation\n",
    "* Big-Omega Notation\n",
    "* Big-Theta Notation\n",
    "\n",
    "### Big-Oh Notation\n",
    "* Gives an upper bound on a function\n",
    "* $T(n)$ is $O(f(n))$ when $n$ is large\n",
    "* Big-O notation concerns functions $T(n)$ defined on the positive integers $n = 1, 2, \\dots$ \n",
    "* $T(n)$ will almost always denote a bound on the worst-case running time of an algorithm\n",
    "\n",
    "$T(n) = O(f(n))$ if and only if $T(n)$ is bounded above by a constant multiple of $f(n)$.\n",
    "\n",
    "*Big-O mathematical expression*\\\n",
    "$\\begin{array}{l}\n",
    "T\\left( n \\right) = O\\left( {f\\left( n \\right)} \\right) \\Leftrightarrow \\exists c,{n_0} s.t.\\forall n \\ge {n_0},0 \\le T\\left( n \\right) \\le c \\cdot f\\left( n \\right)\n",
    "\\end{array}$\n",
    "\n",
    "* $T(n)$ should be bounded above by a multiple of $f(n)$\n",
    "* For all $n\\ge n_0$ expresses that the inequality only needs to hold eventually, once $n$ is sufficiently large (with the constant $n_0$ specifying how large)\n",
    "* Big-O notation helps you understand whether an algorithm will slow down a little or a lot as the problem gets bigger.\n",
    "\n",
    "<img src=images/comp-2.png width=\"400\" height=\"400\">\n",
    "\n",
    "* We essentially say that for sufficiently large inputs, $T(ùëõ)$ does not grow faster than $f(ùëõ)$ multiplied by some constant factor.\n",
    "* Existence of $ùëê$: There exists some constant $ùëê$ which can be used to multiply $T(ùëõ)$ to ensure the inequality holds.\n",
    "* Threshold $ùëõ_0$: There exists a point $ùëõ_0$  beyond which the inequality holds for all $ùëõ\\ge n_0$.\n",
    "\n",
    "<img src=images/comp-3.png width=\"600\" height=\"600\">\n",
    "\n",
    "### Big-Omega Notation\n",
    "* Gives a lower bound on a function\n",
    "* We say $T(n)$ is $\\Omega(f (n))$ when as $n$ gets big, $f (n)$ grows at least as slowly as $T(n)$\n",
    "\n",
    "$\\begin{array}{l}\n",
    "T\\left( n \\right) = \\Omega \\left( {f\\left( n \\right)} \\right) \\Leftrightarrow \\exists c > 0,{n_0} s.t.\\forall n \\ge {n_0},0 \\le c \\cdot f\\left( n \\right) \\le T\\left( n \\right)\n",
    "\\end{array}$\n",
    "\n",
    "### Big-Theta Notation\n",
    "* $\\Theta(\\dots)$  means both\n",
    "* $T(n)$ is $\\Theta(f (n))$ if and only if $T (n) = O(f (n))$ and $T (n) = \\Omega(f (n))$. \n",
    "\n",
    "$\\begin{array}{l}\n",
    "T\\left( n \\right) = \\Theta \\left( {f\\left( n \\right)} \\right) \\Leftrightarrow \\exists {c_1} > 0,{c_2} > 0,{n_0} s.t.\\forall n \\ge {n_0},0 \\le {c_1} \\cdot f\\left( n \\right) \\le T\\left( n \\right) \\le {c_2} \\cdot f\\left( n \\right)\n",
    "\\end{array}$\n",
    "\n",
    "<img src=images/fig7-asymptotic-analysis.jpeg width=\"600\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de618a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
